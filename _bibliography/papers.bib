---
---

@string{isprs = {ISPRS Journal of Photogrammetry and Remote Sensing,}}
@string{cvpr = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),}}

@article{chen2022points2poly,
  abbr={ISPRS P&RS},
  bibtex_show={true},
  title = {Reconstructing compact building models from point clouds using deep implicit fields},
  journal = isprs,
  volume = {194},
  pages = {58-73},
  year = {2022},
  issn = {0924-2716},
  doi = {10.1016/j.isprsjprs.2022.09.017},
  author = {Zhaiyu Chen and Hugo Ledoux and Seyran Khademi and Liangliang Nan},
  abstract={While three-dimensional (3D) building models play an increasingly pivotal role in many real-world applications, 
    obtaining a compact representation of buildings remains an open problem. In this paper, we present a novel framework for 
    reconstructing compact, watertight, polygonal building models from point clouds. Our framework comprises three components: 
    (a) a cell complex is generated via adaptive space partitioning that provides a polyhedral embedding as the candidate set; 
    (b) an implicit field is learned by a deep neural network that facilitates building occupancy estimation; 
    (c) a Markov random field is formulated to extract the outer surface of a building via combinatorial optimization. 
    We evaluate and compare our method with state-of-the-art methods in generic reconstruction, model-based reconstruction, geometry simplification, and primitive assembly. 
    Experiments on both synthetic and real-world point clouds have demonstrated that, with our neural-guided strategy, 
    high-quality building models can be obtained with significant advantages in fidelity, compactness, and computational efficiency. 
    Our method also shows robustness to noise and insufficient measurements, and it can directly generalize from synthetic scans
    to real-world measurements. The source code of this work is freely available at https://github.com/chenzhaiyu/points2poly},
  html={https://www.sciencedirect.com/science/article/pii/S0924271622002611},
  pdf={chen2022points2poly.pdf},
  code={https://github.com/chenzhaiyu/points2poly},
  selected={true},
}

@article{chen2024polygnn,
abbr={ISPRS P&RS},
bibtex_show={true},
title = {PolyGNN: Polyhedron-based graph neural network for 3D building reconstruction from point clouds},
journal = isprs,
volume = {218},
pages = {693-706},
year = {2024},
issn = {0924-2716},
doi = {10.1016/j.isprsjprs.2024.09.031},
author = {Zhaiyu Chen and Yilei Shi and Liangliang Nan and Zhitong Xiong and Xiao Xiang Zhu},
abstract={We present PolyGNN, a polyhedron-based graph neural network for 3D building reconstruction from point clouds. 
  PolyGNN learns to assemble primitives obtained by polyhedral decomposition via graph node classification, 
  achieving a watertight and compact reconstruction. To effectively represent arbitrary-shaped polyhedra in the neural network, 
  we propose a skeleton-based sampling strategy to generate polyhedron-wise queries. These queries are then incorporated 
  with inter-polyhedron adjacency to enhance the classification. PolyGNN is end-to-end optimizable and is designed to 
  accommodate variable-size input points, polyhedra, and queries with an index-driven batching technique. 
  To address the abstraction gap between existing city-building models and the underlying instances, 
  and provide a fair evaluation of the proposed method, we develop our method on a large-scale synthetic dataset with 
  well-defined ground truths of polyhedral labels. We further conduct a transferability analysis across cities and on real-world point clouds. 
  Both qualitative and quantitative results demonstrate the effectiveness of our method, particularly its efficiency for large-scale reconstructions. 
  The source code and data are available at https://github.com/chenzhaiyu/polygnn.},
html={https://www.sciencedirect.com/science/article/pii/S0924271622002611},
pdf={chen2024polygnn.pdf},
code={https://github.com/chenzhaiyu/polygnn},
selected={true},
}

@article{zhao2024gnn,
abbr={GRSM},
bibtex_show={true},
author={Zhao, Shan and Chen, Zhaiyu and Xiong, Zhitong and Shi, Yilei and Saha, Sudipan and Zhu, Xiao Xiang},
journal={IEEE Geoscience and Remote Sensing Magazine}, 
title={Beyond Grid Data: Exploring graph neural networks for Earth observation}, 
year={2024},
volume={},
number={},
pages={2-35},
keywords={Earth;Data models;Graph neural networks;Monitoring;Meteorology;Reviews;Time series analysis;Point cloud compression;Context modeling;Training},
doi={10.1109/MGRS.2024.3493972},
abstract={Earth Observation (EO) data analysis has been significantly revolutionized by deep learning (DL), 
  with applications typically limited to grid-like data structures. Graph Neural Networks (GNNs) emerge as an important innovation, 
  propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively tackle the challenges posed by diverse modalities, 
  multiple sensors, and the heterogeneous nature of EO data. To introduce GNNs in the related domains, our review begins by offering fundamental knowledge on GNNs. 
  Then, we summarize the generic problems in EO, to which GNNs can offer potential solutions. Following this, we explore a broad spectrum of GNNsâ€™ applications to scientific problems in Earth systems, 
  covering areas such as weather and climate analysis, disaster management, air quality monitoring, agriculture, land cover classification, hydrological process modeling, and urban modeling. 
  The rationale behind adopting GNNs in these fields is explained, alongside methodologies for organizing graphs and designing favorable architectures for various tasks. 
  Furthermore, we highlight methodological challenges of implementing GNNs in these domains and possible solutions that could guide future research. 
  While acknowledging that GNNs are not a universal solution, we conclude the paper by comparing them with other popular architectures like transformers and analyzing their potential synergies.},
selected={true},
}

@article{chen2025paco,
abbr={CVPR},
bibtex_show={true},
title = {Parametric Point Cloud Completion for Polygonal Surface Reconstruction},
journal = cvpr,
year = {2025},
author = {Zhaiyu Chen and Yuqing Wang and Liangliang Nan and Xiao Xiang Zhu},
abstract={Existing polygonal surface reconstruction methods heavily depend on input completeness and struggle with incomplete point clouds. 
We argue that while current point cloud completion techniques may recover missing points, they are not optimized for polygonal surface reconstruction, 
where the parametric representation of underlying surfaces remains overlooked. To address this gap, we introduce parametric completion, 
a novel paradigm for point cloud completion, which recovers parametric primitives instead of individual points to convey high-level geometric structures. 
Our presented approach, PaCo, enables high-quality polygonal surface reconstruction by leveraging plane proxies that encapsulate both plane parameters and inlier points, 
proving particularly effective in challenging scenarios with highly incomplete data. Comprehensive evaluations of our approach on the ABC dataset 
establish its effectiveness with superior performance and set a new standard for polygonal surface reconstruction from incomplete data.},
pdf={chen2025paco.pdf},
code={https://github.com/PaCo-X/PaCo},
website={https://parametric-completion.github.io},
selected={true},
}
